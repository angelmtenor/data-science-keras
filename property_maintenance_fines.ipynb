{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Property Maintenance Fines \n",
    "\n",
    "**Predicting the probability that a set of blight tickets will be paid on time**\n",
    "\n",
    "**Supervised Learning. Classification**\n",
    "\n",
    "Source: [Applied Machine Learning in Python | Coursera](https://www.coursera.org/learn/python-machine-learning). Solved with classical machine learning classifiers [here](https://github.com/angelmtenor/applied-ML-coursera/blob/master/4_property_maintenance_fines.ipynb)\n",
    "\n",
    "Data provided by Michigan Data Science Team ([MDST](http://midas.umich.edu/mdst/)), the Michigan Student Symposium for Interdisciplinary Statistical Sciences ([MSSISS](https://sites.lsa.umich.edu/mssiss/)) and the City of Detroit [Detroit Open Data Portal](https://data.detroitmi.gov/).\n",
    " \n",
    "\n",
    "Each row of the dataset   corresponds to a single blight ticket, and includes information about when, why, and to whom each ticket was issued. The target variable is compliance, which is True if the ticket was paid early, on time, or within one month of the hearing data, False if the ticket was paid after the hearing date or not at all, and Null if the violator was found not responsible. \n",
    "\n",
    "**Features**\n",
    "    \n",
    "    ticket_id - unique identifier for tickets\n",
    "    agency_name - Agency that issued the ticket\n",
    "    inspector_name - Name of inspector that issued the ticket\n",
    "    violator_name - Name of the person/organization that the ticket was issued to\n",
    "    violation_street_number, violation_street_name, violation_zip_code - Address where the violation occurred\n",
    "    mailing_address_str_number, mailing_address_str_name, city, state, zip_code, non_us_str_code, country - Mailing address of the violator\n",
    "    ticket_issued_date - Date and time the ticket was issued\n",
    "    hearing_date - Date and time the violator's hearing was scheduled\n",
    "    violation_code, violation_description - Type of violation\n",
    "    disposition - Judgment and judgement type\n",
    "    fine_amount - Violation fine amount, excluding fees\n",
    "    admin_fee - $20 fee assigned to responsible judgments\n",
    "state_fee - $10 fee assigned to responsible judgments\n",
    "    late_fee - 10% fee assigned to responsible judgments\n",
    "    discount_amount - discount applied, if any\n",
    "    clean_up_cost - DPW clean-up or graffiti removal cost\n",
    "    judgment_amount - Sum of all fines and fees\n",
    "    grafitti_status - Flag for graffiti violations\n",
    "    \n",
    "** Labels **\n",
    "\n",
    "    payment_amount - Amount paid, if any\n",
    "    payment_date - Date payment was made, if it was received\n",
    "    payment_status - Current payment status as of Feb 1 2017\n",
    "    balance_due - Fines and fees still owed\n",
    "    collection_status - Flag for payments in collections\n",
    "    compliance [target variable for prediction]\n",
    "     Null = Not responsible\n",
    "     0 = Responsible, non-compliant\n",
    "     1 = Responsible, compliant\n",
    "    compliance_detail - More information on why each ticket was marked compliant or non-compliant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import helper_ml\n",
    "import keras\n",
    "\n",
    "helper_ml.info_system()\n",
    "# sns.set_palette(\"GnBu_d\")\n",
    "# helper_ml.reproducible(seed=0) # Setup reproducible results from run to run using Keras\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1. Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/property_maintenance_fines_data.parquet\"\n",
    "target = [\"compliance\"]\n",
    "\n",
    "# df_original = pd.read_csv(data_path, encoding=\"iso-8859-1\", dtype=\"unicode\")\n",
    "df_original = pd.read_parquet(data_path)  # , encoding=\"iso-8859-1\", dtype=\"unicode\")\n",
    "\n",
    "print(\"{} rows \\n{} columns \\ntarget: {}\".format(*df_original.shape, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_original.to_parquet(\"data/property_maintenance_fines_data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore and Clean the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_original[target].squeeze().value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with NULL targets\n",
    "\n",
    "df_original = df_original.dropna(subset=target)\n",
    "\n",
    "print(df_original[target].squeeze().value_counts())\n",
    "print(df_original.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imbalanced target: the evaluation metric used in this problem is the Area Under the ROC Curve**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Split original data into training and validation test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df, df_test = train_test_split(df_original, test_size=0.2, stratify=df_original[target], random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To avoid data leakage, only the training dataframe, df, will be explored and processed here** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_ml.missing(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove irrelevant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_features(df):\n",
    "\n",
    "    relevant_col = [\n",
    "        \"agency_name\",\n",
    "        \"violation_street_name\",\n",
    "        \"city\",\n",
    "        \"state\",\n",
    "        \"violator_name\",\n",
    "        \"violation_code\",\n",
    "        \"late_fee\",\n",
    "        \"discount_amount\",\n",
    "        \"judgment_amount\",\n",
    "        \"disposition\",\n",
    "        \"fine_amount\",\n",
    "        \"compliance\",\n",
    "    ]\n",
    "\n",
    "    df = df[relevant_col]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df = remove_features(df)\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classify variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = [\"late_fee\", \"discount_amount\", \"judgment_amount\", \"fine_amount\"]\n",
    "\n",
    "df = helper_ml.classify_data(df, target, numerical=num)\n",
    "\n",
    "pd.DataFrame(dict(df.dtypes), index=[\"Type\"])[df.columns].head()  # show data types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove low-frequency categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, dict_categories = helper_ml.remove_categories(df, target=target, ratio=0.001, show=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Fill missing values\n",
    "\n",
    "Missing categorical values filled by 'Other'\n",
    "There are no numerical missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = helper_ml.fill_simple(df, target, missing_categorical=\"Other\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_ml.missing(df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [\"state\", \"disposition\"]:\n",
    "    helper_ml.show_categorical(df[[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target vs Categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in [\"state\", \"disposition\"]:\n",
    "#     helper_ml.show_target_vs_categorical(df[[i, target[0]]], target) # TODO Fix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_ml.show_numerical(df, kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target vs Numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper_ml.show_target_vs_numerical(df, target, point_size=10, jitter=0.3, fit_reg=True)\n",
    "# plt.ylim(ymin=-0.2, ymax=1.2)  # TODO: Optimize (high computation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation between numerical features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_ml.correlation(df, target, figsize=(6, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "droplist = []  # features to drop\n",
    "\n",
    "# For the model 'data' instead of 'df'\n",
    "data = df.copy()\n",
    "# del(df)\n",
    "data.drop(droplist, axis=\"columns\", inplace=True)\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale numerical variables\n",
    "Shift and scale numerical variables to a standard normal distribution. The scaling factors are saved to be used for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, scale_param = helper_ml.scale(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dummy features\n",
    "Replace categorical features (no target) with dummy features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, dict_dummies = helper_ml.replace_by_dummies(data, target)  # TODO: Optimize (high computation)\n",
    "\n",
    "model_features = [f for f in data if f not in target]  # sorted neural network inputs\n",
    "\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size = 0.2\n",
    "random_state = 0\n",
    "\n",
    "\n",
    "def validation_split(data, val_size=0.25):\n",
    "\n",
    "    train, test = train_test_split(data, test_size=val_size, random_state=random_state, stratify=data[target])\n",
    "\n",
    "    # Separate the data into features and target (x=features, y=target)\n",
    "    x_train, y_train = train.drop(target, axis=1).values, train[target].values\n",
    "    x_val, y_val = test.drop(target, axis=1).values, test[target].values\n",
    "    # _nc: non-categorical yet (needs one-hot encoding)\n",
    "\n",
    "    return x_train, y_train, x_val, y_val\n",
    "\n",
    "\n",
    "x_train, y_train, x_val, y_val = validation_split(data, val_size=val_size)\n",
    "\n",
    "# x_train = x_train.astype(np.float16)\n",
    "y_train = y_train.astype(np.float16)\n",
    "# X_val = x_val.astype(np.float16)\n",
    "y_val = y_val.astype(np.float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_output(y_train, y_val):\n",
    "    num_classes = len(np.unique(y_train))\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_val = keras.utils.to_categorical(y_val, num_classes)\n",
    "    return y_train, y_val\n",
    "\n",
    "\n",
    "y_train, y_val = one_hot_output(y_train, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train size \\t X:{} \\t Y:{}\".format(x_train.shape, y_train.shape))\n",
    "print(\"val size \\t X:{} \\t Y:{}\".format(x_val.shape, y_val.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a dummy classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "clf = DummyClassifier(strategy=\"most_frequent\").fit(x_train, np.ravel(y_train))\n",
    "# The dummy 'most_frequent' classifier always predicts class 0\n",
    "y_pred = clf.predict(x_val).reshape([-1, 1])\n",
    "\n",
    "helper_ml.binary_classification_scores(y_val[:, 1], y_pred);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a random forest classifier (best of grid search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "%time clf_random_forest_opt = RandomForestClassifier(n_estimators = 30, max_features=150, \\\n",
    "                                max_depth=13, class_weight='balanced', n_jobs=-1, \\\n",
    "                                   random_state=0).fit(x_train, np.ravel(y_train[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf_random_forest_opt.predict(x_val).reshape([-1, 1])\n",
    "helper_ml.binary_classification_scores(y_val[:, 1], y_pred);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Neural Network for Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cw = helper_ml.get_class_weight(y_train[:, 1])  # class weight (imbalanced target)\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout\n",
    "\n",
    "\n",
    "def build_nn(input_size, output_size, summary=False):\n",
    "\n",
    "    input_nodes = input_size // 8\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(input_nodes, input_dim=input_size, activation=\"relu\"))\n",
    "\n",
    "    model.add(Dense(output_size, activation=\"softmax\"))\n",
    "\n",
    "    if summary:\n",
    "        model.summary()\n",
    "\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_nn(x_train.shape[1], y_train.shape[1], summary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import time\n",
    "\n",
    "model_path = os.path.join(\"models\", \"detroit.h5\")\n",
    "\n",
    "\n",
    "def train_nn(model, x_train, y_train, validation_data=None, path=False, show=True):\n",
    "    \"\"\"\n",
    "    Train the neural network model. If no validation_data is provided, a split for validation\n",
    "    will be used\n",
    "    \"\"\"\n",
    "\n",
    "    if show:\n",
    "        print(\"Training ....\")\n",
    "\n",
    "    callbacks = [keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=0, verbose=1)]\n",
    "    t0 = time()\n",
    "\n",
    "    history = model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        epochs=100,\n",
    "        batch_size=2048,\n",
    "        class_weight=cw,\n",
    "        verbose=1,\n",
    "        validation_split=0.3,\n",
    "        validation_data=validation_data,\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "\n",
    "    if show:\n",
    "        print(\"time: \\t {:.1f} s\".format(time() - t0))\n",
    "        helper_ml.show_training(history)\n",
    "\n",
    "    if path:\n",
    "        model.save(path)\n",
    "        print(\"\\nModel saved at\", path)\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "model = None\n",
    "model = build_nn(x_train.shape[1], y_train.shape[1], summary=False)\n",
    "train_nn(model, x_train, y_train, path=None)\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "y_pred_train = model.predict(x_train, verbose=1)\n",
    "print(\"\\n\\n ROC_AUC train:\\t{:.2f} \\n\".format(roc_auc_score(y_train, y_pred_train)))\n",
    "y_pred_val = model.predict(x_val, verbose=1)\n",
    "print(\"\\n\\n ROC_AUC val:\\t{:.2f}\".format(roc_auc_score(y_val, y_pred_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate the model (validation set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_ml.binary_classification_scores(y_val[:, 1], y_pred_val[:, 1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the final model (test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process test data with training set parameters (no data leakage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = remove_features(df_test)\n",
    "\n",
    "df_test = helper_ml.classify_data(df_test, target, numerical=num)\n",
    "\n",
    "df_test, _ = helper_ml.remove_categories(df_test, target=target, show=False, dict_categories=dict_categories)\n",
    "\n",
    "df_test = helper_ml.fill_simple(df_test, target, missing_categorical=\"Other\")\n",
    "\n",
    "df_test, _ = helper_ml.scale(df_test, scale_param)\n",
    "df_test, _ = helper_ml.replace_by_dummies(df_test, target, dict_dummies)\n",
    "df_test = df_test[model_features + target]  # sort columns to match training features order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_x_y(data):\n",
    "    \"\"\"Separate the data into features and target (x=features, y=target)\"\"\"\n",
    "\n",
    "    x, y = data.drop(target, axis=1).values, data[target].values\n",
    "    x = x.astype(np.float16)\n",
    "    y = y.astype(np.float16)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "x_test, y_test = separate_x_y(df_test)\n",
    "\n",
    "y_test = keras.utils.to_categorical(y_test, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf_random_forest_opt.predict_proba(x_test)[:, 1]\n",
    "helper_ml.binary_classification_scores(y_test[:, 1], y_pred);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_ml.show_feature_importance(model_features, clf_random_forest_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test, verbose=1)[:, 1]\n",
    "helper_ml.binary_classification_scores(y_test[:, 1], y_pred);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare with other non-neural ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_ml.ml_classification(x_train, y_train[:, 1], x_test, y_test[:, 1])"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "python-machine-learning",
   "graded_item_id": "nNS8l",
   "launcher_item_id": "yWWk7",
   "part_id": "w8BSS"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ds-keras')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "366a4202e041eebdd8a4edd8a0024926c2e45e658d7ba92f3c9b74c9b0ce97a9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
