{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true
      },
      "source": [
        "# Machine Translation\n",
        "\n",
        "**Recurrent Neural Network that accepts English text as input and returns the French translation**\n",
        "\n",
        "**Natural Language Processing**\n",
        "\n",
        "This notebook is based on the Natural Language Processing [capstone project](https://github.com/udacity/aind2-nlp-capstone) of the [Udacity's Artificial Intelligence  Nanodegree](https://www.udacity.com/course/artificial-intelligence-nanodegree--nd889).\n",
        "\n",
        "The dataset is a reduced sentence set taken from [WMT](http://www.statmt.org/). The `small_vocab_en` file contains English sentences with their French translations in the `small_vocab_fr` file.  The punctuations have been delimited using spaces already, and all the text have been converted to lowercase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, sys\n",
        "import numpy as np\n",
        "\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "sys.path.append(\"../\")\n",
        "import ds_boost\n",
        "from tensorflow import keras\n",
        "\n",
        "log = ds_boost.logger.init(level=\"DEBUG\", save_log=False)\n",
        "\n",
        "ds_boost.set_parent_execution_path()\n",
        "ds_boost.info_system()\n",
        "np.random.seed(9)\n",
        "\n",
        "%matplotlib inline\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load and prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"data/small_vocab_en\", \"r\") as f:\n",
        "    english_sentences = f.read().split(\"\\n\")\n",
        "with open(\"data/small_vocab_fr\", \"r\") as f:\n",
        "    french_sentences = f.read().split(\"\\n\")\n",
        "\n",
        "print(f\"Number of sentences: {len(english_sentences)}\\n\")\n",
        "for i in range(2):\n",
        "    print(f\"sample {i}:\")\n",
        "    print(f\"{english_sentences[i]}  \\n{french_sentences[i]} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import collections\n",
        "\n",
        "words = {}\n",
        "words[\"English\"] = [word for sentence in english_sentences for word in sentence.split()]\n",
        "words[\"French\"] = [word for sentence in french_sentences for word in sentence.split()]\n",
        "\n",
        "for key, value in words.items():\n",
        "    print(f\"{key}: {len(value)} words, {len(collections.Counter(value))} unique words\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tokenize\n",
        "Low complexity word to numerical word ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "\n",
        "def tokenize(x):\n",
        "    \"\"\"\n",
        "    :param x: List of sentences/strings to be tokenized\n",
        "    :return: Tuple of (tokenized x data, tokenizer used to tokenize x)\n",
        "    \"\"\"\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(x)\n",
        "    tokens = tokenizer.texts_to_sequences(x)\n",
        "\n",
        "    return tokens, tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Padding\n",
        "When batching the sequence of word ids together, each sequence needs to be the same length.  Since sentences are dynamic in length, we can add padding to the end of the sequences to make them the same length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import pad_sequences\n",
        "\n",
        "\n",
        "def pad(x, length=None):\n",
        "    \"\"\"\n",
        "    :param x: List of sequences.\n",
        "    :param length: Length to pad the sequence to. If None, longest sequence length in x.\n",
        "    :return: Padded numpy array of sequences\n",
        "    \"\"\"\n",
        "    return pad_sequences(x, maxlen=length, padding=\"post\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocess pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess(x, y, length=None):\n",
        "    \"\"\"\n",
        "    :param x: Feature List of sentences\n",
        "    :param y: Label List of sentences\n",
        "    :return: Tuple of (Preprocessed x, Preprocessed y, x tokenizer, y tokenizer)\n",
        "    \"\"\"\n",
        "    preprocess_x, x_tk = tokenize(x)\n",
        "    preprocess_y, y_tk = tokenize(y)\n",
        "\n",
        "    preprocess_x = pad(preprocess_x, length)\n",
        "    preprocess_y = pad(preprocess_y, length)\n",
        "\n",
        "    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dims\n",
        "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
        "\n",
        "    return preprocess_x, preprocess_y, x_tk, y_tk\n",
        "\n",
        "\n",
        "x, y, x_tk, y_tk = preprocess(english_sentences, french_sentences)  # length=150)\n",
        "print(\"Data Preprocessed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Split the data into training and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Only the 10 last translations will be predicted\n",
        "x_train, y_train = x[:-10], y[:-10]\n",
        "x_test, y_test = x[-10:-1], y[-10:-1]  # last sentence removed\n",
        "test_english_sentences, test_french_sentences = (\n",
        "    english_sentences[-10:],\n",
        "    french_sentences[-10:],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ids Back to Text\n",
        "The function `logits_to_text` will bridge the gap between the logits from the neural network to the French translation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def logits_to_text(logits, tokenizer, show_pad=True):\n",
        "    \"\"\"\n",
        "    Turn logits from a neural network into text using the tokenizer\n",
        "    :param logits: Logits from a neural network\n",
        "    :param tokenizer: Keras Tokenizer fit on the labels\n",
        "    :return: String that represents the text of the logits\n",
        "    \"\"\"\n",
        "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
        "    index_to_words[0] = \"<PAD>\" if show_pad else \"\"\n",
        "\n",
        "    return \" \".join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Recurrent neural network\n",
        "Model that incorporates encoder-decoder, embedding and bidirectional RNNs: \n",
        "- An embedding is a vector representation of the word that is close to similar words in $n$-dimensional space, where the $n$ represents the size of the embedding vectors \n",
        "- The encoder creates a matrix representation of the sentence\n",
        "- The decoder takes this matrix as input and predicts the translation as output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import GRU, Dense, TimeDistributed, LSTM, Bidirectional, RepeatVector\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from keras.layers import Dropout\n",
        "from keras.losses import sparse_categorical_crossentropy\n",
        "\n",
        "\n",
        "def rnn_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
        "    \"\"\"\n",
        "    Build a model with embedding, encoder-decoder, and bidirectional RNN\n",
        "    :param input_shape: Tuple of input shape\n",
        "    :param output_sequence_length: Length of output sequence\n",
        "    :param english_vocab_size: Number of unique English words in the dataset\n",
        "    :param french_vocab_size: Number of unique French words in the dataset\n",
        "    :return: Keras model built, but not trained\n",
        "    \"\"\"\n",
        "    learning_rate = 0.01\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    vector_size = english_vocab_size // 10\n",
        "\n",
        "    model.add(\n",
        "        Embedding(\n",
        "            english_vocab_size + 1,\n",
        "            vector_size,\n",
        "            input_shape=input_shape[1:],\n",
        "            mask_zero=False,\n",
        "        )\n",
        "    )\n",
        "    model.add(Bidirectional(GRU(output_sequence_length)))\n",
        "    model.add(Dense(128, activation=\"relu\"))\n",
        "\n",
        "    model.add(RepeatVector(output_sequence_length))\n",
        "    model.add(Bidirectional(GRU(128, return_sequences=True)))\n",
        "    model.add(TimeDistributed(Dense(french_vocab_size + 1, activation=\"softmax\")))\n",
        "    print(model.summary())\n",
        "\n",
        "    model.compile(\n",
        "        loss=sparse_categorical_crossentropy,\n",
        "        optimizer=keras.optimizers.Adam(learning_rate),\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "model = rnn_model(x_train.shape, y_train.shape[1], len(x_tk.word_index), len(y_tk.word_index))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Training...\")\n",
        "callbacks = [keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=3, verbose=1)]\n",
        "history = model.fit(x_train, y_train, batch_size=1024, epochs=50, verbose=0, validation_split=0.2, callbacks=callbacks)\n",
        "ds_boost.show_training(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(f\"Test Accuracy: {score[1]:.2f}\\n\")\n",
        "\n",
        "y = model.predict(x_test)\n",
        "\n",
        "for idx, value in enumerate(y):\n",
        "    print(f\"Sample: {test_english_sentences[idx]}\")\n",
        "    print(f\"Actual: {test_french_sentences[idx]}\")\n",
        "    print(f\"Predicted: {logits_to_text(value, y_tk, show_pad=False)}\\n\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.4 ('ds-keras')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "366a4202e041eebdd8a4edd8a0024926c2e45e658d7ba92f3c9b74c9b0ce97a9"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
