{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bike Rental\n",
    "\n",
    "**Daily bike rental ridership prediction using an artificial neural network in Keras**\n",
    "\n",
    "**Supervised Learning. Regression**\n",
    "\n",
    "\n",
    "Based on the [first neural network project](https://github.com/udacity/deep-learning/tree/master/first-neural-network) from the [Deep Learning Nanodegree Foundation of Udacity](https://www.udacity.com/course/deep-learning-nanodegree-foundation--nd101)\n",
    "\n",
    "Click [Here](https://github.com/angelmtenor/deep-learning/blob/master/first-neural-network/dlnd-your-first-neural-network.ipynb) to check my original solution in Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import helper\n",
    "import keras\n",
    "\n",
    "helper.info_gpu()\n",
    "sns.set_palette(\"Reds\")\n",
    "helper.reproducible(seed=0)  # setup reproducible results from run to run using Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Processing and Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/Bike-Sharing-Dataset/hour.csv\"\n",
    "target = [\"cnt\", \"casual\", \"registered\"]\n",
    "\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.info_data(df, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numerical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(percentiles=[0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.missing(df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = pd.to_datetime(df[\"dteday\"])\n",
    "df[\"day\"] = pd.DatetimeIndex(ind).day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove irrelevant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "droplist = [\"atemp\", \"day\", \"instant\"]\n",
    "df.drop(droplist, axis=\"columns\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classify variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical = [\"temp\", \"hum\", \"windspeed\", \"cnt\", \"casual\", \"registered\"]\n",
    "\n",
    "df = helper.classify_data(df, target, numerical)\n",
    "\n",
    "helper.get_types(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the data\n",
    "\n",
    "This dataset has the number of riders for each hour of each day from January 1 2011 to December 31 2012. The number of riders is split between casual and registered, summed up in the `cnt` column. We also have information about temperature, humidity, and windspeed, all of these likely affecting the number of riders. Below is a plot showing the hourly rentals over the first 10 days in the data set. The weekends have lower over all ridership and there are spikes when people are biking to and from work during the week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[: 24 * 10].plot(x=\"dteday\", y=\"cnt\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.show_categorical(df[[\"holiday\", \"workingday\", \"weathersit\"]], sharey=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target vs Categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.show_target_vs_categorical(df.drop([\"dteday\", \"season\"], axis=\"columns\"), target, ncols=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.PairGrid(df, y_vars=\"casual\", x_vars=\"registered\", size=5, aspect=9 / 4, hue=\"weathersit\")\n",
    "g.map(sns.regplot, fit_reg=False).add_legend()\n",
    "g.axes[0, 0].set_ylim(0, 350)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot shows the differences between the number of registered and casual riders for the different weather situations. Most of the riders are registered with very bad weather (`weathersit=4`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.show_numerical(df, kde=True, ncols=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target vs numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.show_target_vs_numerical(df, target, jitter=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation between numerical features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.correlation(df, target, figsize=(10, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Neural Network model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "droplist = [\"dteday\"]  # features to drop from the model\n",
    "\n",
    "# For the model 'data' instead of 'df'\n",
    "data = df.copy()\n",
    "data.drop(droplist, axis=\"columns\", inplace=True)\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale numerical variables\n",
    "\n",
    "Shift and scale numerical variables to a standard normal distribution. The scaling factors are saved to be used for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, scale_param = helper.scale(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dummy features\n",
    "Replace categorical features (no target) with dummy features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, dict_dummies = helper.replace_by_dummies(data, target)\n",
    "\n",
    "model_features = [f for f in data if f not in target]  # sorted neural network inputs\n",
    "\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into training, validation, and test sets\n",
    "Data leakage: Test set hidden when training the model, but seen when preprocessing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the last 21 days as a test set\n",
    "test = data[-21 * 24 :]\n",
    "train = data[: -21 * 24]\n",
    "\n",
    "# Hold out the last 60 days of the remaining data as a validation set\n",
    "val = train[-60 * 24 :]\n",
    "train = train[: -60 * 24]\n",
    "\n",
    "# Separate the data into features(x) and targets(y)\n",
    "x_train, y_train = train.drop(target, axis=1).values, train[target].values\n",
    "x_val, y_val = val.drop(target, axis=1).values, val[target].values\n",
    "x_test, y_test = test.drop(target, axis=1).values, test[target].values\n",
    "\n",
    "print(\"train size \\t X:{} \\t Y:{}\".format(x_train.shape, y_train.shape))\n",
    "print(\"val size \\t X:{} \\t Y:{}\".format(x_val.shape, y_val.shape))\n",
    "print(\"test size  \\t X:{} \\t Y:{} \".format(x_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = helper.build_nn_reg(x_train.shape[1], y_train.shape[1], hidden_layers=2, dropout=0.2, summary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(\"models\", \"bike_rental.h5\")\n",
    "\n",
    "model = None\n",
    "model = helper.build_nn_reg(x_train.shape[1], y_train.shape[1], hidden_layers=2, dropout=0.2, summary=True)\n",
    "\n",
    "callbacks = [keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=2, verbose=0)]\n",
    "\n",
    "helper.train_nn(\n",
    "    model,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    validation_data=[x_val, y_val],\n",
    "    path=model_path,\n",
    "    epochs=100,\n",
    "    batch_size=2048,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "ypred_train = model.predict(x_train)\n",
    "ypred_val = model.predict(x_val)\n",
    "print(\"\\nTraining   R2-score: \\t{:.3f}\".format(r2_score(y_train, ypred_train)))\n",
    "print(\"Validation R2-score: \\t{:.3f}\".format(r2_score(y_val, ypred_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = model.predict(x_test, verbose=0)\n",
    "helper.regression_scores(y_test, y_pred_test, return_dataframe=True, index=\"DNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "\n",
    "mean, std = scale_param[\"cnt\"]\n",
    "predictions = y_pred_test * std + mean\n",
    "ax.plot(predictions[:, 0], label=\"Prediction\")\n",
    "ax.plot((test[\"cnt\"] * std + mean).values, label=\"Data\")\n",
    "ax.set_xlim(right=len(predictions))\n",
    "ax.legend()\n",
    "\n",
    "dates = pd.to_datetime(df.iloc[test.index][\"dteday\"])\n",
    "dates = dates.apply(lambda d: d.strftime(\"%b %d\"))\n",
    "ax.set_xticks(np.arange(len(dates))[12::24])\n",
    "_ = ax.set_xticklabels(dates[12::24], rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model seems quite accurate considering that only two years of data were available.\n",
    "\n",
    "It fails on the last 10 days of December where we expected more bike riders.\n",
    "\n",
    "The model was not trained to predict this fall. The training set included data from December 22 to December 31 from one year only (2011), which is not enough. An exploratory analysis and some tests with different models led me to the following results and conclusions:\n",
    "    \n",
    "- Adding more features from the dataset has a negligible impact on the accuracy of the model, only increasing the size of the neural network. \n",
    "\n",
    "- Removing or replacing the current features makes the model worse.\n",
    "\n",
    "- The training period December 22 to December 31 in 2011 had more registered riders (mean = 73.6) than the test period in 2012 (mean = 58.3). A ridership drop on Christmas 2012 can be predicted from the weather (worse than 2011), but not the large decline registered. Adding new features could help solve this issue, such as *active registrations* or *Christmas*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare with classical ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore training set\n",
    "x_train = np.vstack([x_train, x_val])\n",
    "y_train = np.vstack([y_train, y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.ml_regression(x_train, y_train[:, 0], x_test, y_test[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Best tree-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "random_forest = RandomForestRegressor(n_jobs=-1, n_estimators=100, random_state=0).fit(x_train, np.ravel(y_train[:, 0]))\n",
    "\n",
    "y_pred = random_forest.predict(x_test)\n",
    "\n",
    "helper.regression_scores(y_test[:, 0], y_pred, return_dataframe=True, index=\"Random Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = helper.feature_importance(model_features, random_forest)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.10.4 ('gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "d2efdb989dc164e4378df2c802d4acdf5890bc9f1d40e64af3f699a110ebaaa8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
