{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting House Prices\n",
    "\n",
    "**Sales prices prediction using an artificial neural network in Keras**\n",
    "\n",
    "**Supervised Learning. Regression**\n",
    "\n",
    "Source: Ames Housing dataset ([Kaggle website](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import helper\n",
    "import keras\n",
    "\n",
    "helper.info_gpu()\n",
    "sns.set_palette(\"GnBu_d\")\n",
    "# helper.reproducible(seed=0) # Setup reproducible results from run to run using Keras\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Processing and Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/house_prices_data.csv\"\n",
    "target = [\"SalePrice\"]\n",
    "\n",
    "df_original = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.info_data(df_original, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_missing = helper.missing(df_original, limit=0.4, plot=True)\n",
    "high_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove irrelevant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_original.copy()  # modified dataset\n",
    "\n",
    "# remove non-significant and high-missing features\n",
    "droplist = [\"Id\"] + high_missing\n",
    "\n",
    "assert len(set(droplist).intersection(set(target))) == 0, \"Targets cannot be dropped\"\n",
    "\n",
    "df.drop(droplist, axis=\"columns\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classify variables\n",
    "\n",
    "Change categorical variables as dtype 'categorical' and sort columns: numerical + categorical + target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical = list(df.select_dtypes(include=[np.number]))\n",
    "\n",
    "df = helper.classify_data(df, target, numerical=numerical)\n",
    "\n",
    "helper.get_types(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove low frequency values from categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, dict_categories = helper.remove_categories(df, target, ratio=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill missing values\n",
    "Numerical -> median, categorical -> mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.fill_simple(df, target, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target vs some significant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.PairGrid(df, y_vars=[\"SalePrice\"], x_vars=[\"LotArea\", \"YearBuilt\"], size=5, hue=\"OverallQual\")\n",
    "g.map(plt.scatter).add_legend()\n",
    "g.axes[0, 0].set_xlim(0, 20000)\n",
    "plt.ylim(df[\"SalePrice\"].min(), 600000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lower sale prices are usually found in very low overall quality houses, with less dependency on its size and the year of construction. These three features alone are insufficient to make a good price prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.show_categorical(df, sharey=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target vs Categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.show_target_vs_categorical(df, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.show_numerical(df, kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target vs Numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.show_target_vs_numerical(df, target, point_size=20, jitter=0.2, fit_reg=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation between numerical features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.correlation(df, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Neural Network model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "droplist = []  # features to drop\n",
    "\n",
    "# For the model 'data' instead of 'df'\n",
    "data = df.copy()\n",
    "data.drop(droplist, axis=\"columns\", inplace=True)\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale numerical variables\n",
    "Shift and scale numerical variables to a standard normal distribution. The scaling factors are saved to be used for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, scale_param = helper.scale(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dummy features\n",
    "Replace categorical features (no target) with dummy features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, dict_dummies = helper.replace_by_dummies(data, target)\n",
    "\n",
    "model_features = [f for f in data if f not in target]  # sorted neural network inputs\n",
    "\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into training, validation, and test sets\n",
    "Data leakage: Test set hidden when training the model, but seen when preprocessing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "val_size = 0.1\n",
    "random_state = 9\n",
    "\n",
    "x_train, y_train, x_val, y_val, x_test, y_test = helper.train_val_test_split(\n",
    "    data, target, test_size=test_size, val_size=val_size, random_state=random_state\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encode the output not needed for regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Train the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(\"models\", \"house_prices.h5\")\n",
    "\n",
    "weights = weights = keras.initializers.TruncatedNormal(stddev=0.0001)\n",
    "opt = keras.optimizers.adam(lr=0.00005)\n",
    "\n",
    "model = None\n",
    "model = helper.build_nn_reg(\n",
    "    x_train.shape[1],\n",
    "    y_train.shape[1],\n",
    "    hidden_layers=1,\n",
    "    input_nodes=x_train.shape[1] // 2,\n",
    "    dropout=0.2,\n",
    "    kernel_initializer=weights,\n",
    "    bias_initializer=weights,\n",
    "    optimizer=opt,\n",
    "    summary=True,\n",
    ")\n",
    "\n",
    "callbacks = [keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=3, verbose=0)]\n",
    "\n",
    "helper.train_nn(\n",
    "    model,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    validation_data=[x_val, y_val],\n",
    "    path=model_path,\n",
    "    epochs=500,\n",
    "    batch_size=16,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "ypred_train = model.predict(x_train)\n",
    "ypred_val = model.predict(x_val)\n",
    "print(\"Training   R2-score: \\t{:.3f}\".format(r2_score(y_train, ypred_train)))\n",
    "print(\"Validation R2-score: \\t{:.3f}\".format(r2_score(y_val, ypred_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore training set\n",
    "x_train = np.vstack((x_train, x_val))\n",
    "y_train = np.vstack((y_train, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "def cv_train_nn(x_train, y_train, n_splits):\n",
    "    \"\"\"Create and Train models for cross validation. Return best model\"\"\"\n",
    "\n",
    "    skf = KFold(n_splits=n_splits, shuffle=True)\n",
    "\n",
    "    score = []\n",
    "\n",
    "    best_model = None\n",
    "    best_loss = float(\"inf\")\n",
    "\n",
    "    print(\"Training {} models for Cross Validation ...\".format(n_splits))\n",
    "\n",
    "    for train, val in skf.split(x_train[:, 0], y_train[:, 0]):\n",
    "        model = None\n",
    "        model = helper.build_nn_reg(\n",
    "            x_train.shape[1],\n",
    "            y_train.shape[1],\n",
    "            hidden_layers=1,\n",
    "            input_nodes=x_train.shape[1] // 2,\n",
    "            dropout=0.2,\n",
    "            kernel_initializer=weights,\n",
    "            bias_initializer=weights,\n",
    "            optimizer=opt,\n",
    "            summary=False,\n",
    "        )\n",
    "\n",
    "        history = helper.train_nn(\n",
    "            model,\n",
    "            x_train[train],\n",
    "            y_train[train],\n",
    "            show=False,\n",
    "            validation_data=(x_train[val], y_train[val]),\n",
    "            epochs=500,\n",
    "            batch_size=16,\n",
    "            callbacks=callbacks,\n",
    "        )\n",
    "\n",
    "        val_loss = history.history[\"val_loss\"][-1]\n",
    "\n",
    "        score.append(val_loss)\n",
    "\n",
    "        if val_loss < best_loss:  # save best model (fold) for evaluation and predictions\n",
    "            best_model = model\n",
    "            best_loss = val_loss\n",
    "\n",
    "    print(\"\\nCross Validation loss: {:.3f}\".format(np.mean(score)))\n",
    "    return best_model\n",
    "\n",
    "\n",
    "model = cv_train_nn(x_train, y_train, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = model.predict(x_test, verbose=0)\n",
    "helper.regression_scores(y_test, y_pred_test, return_dataframe=True, index=\"DNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_nn(model, x_test, target):\n",
    "    \"\"\"Return a dataframe with actual and predicted targets in original scale\"\"\"\n",
    "\n",
    "    for t in target:\n",
    "        pred = model.predict(x_test, verbose=0)\n",
    "        restore_pred = pred * scale_param[t][1] + scale_param[t][0]\n",
    "        restore_pred = restore_pred.round()\n",
    "\n",
    "        restore_y = y_test * scale_param[t][1] + scale_param[t][0]\n",
    "        restore_y = restore_y.round()\n",
    "\n",
    "        pred_label = \"Predicted_\" + t\n",
    "        error_label = t + \" error (%)\"\n",
    "\n",
    "        pred_df = pd.DataFrame({t: np.squeeze(restore_y), pred_label: np.squeeze(restore_pred)})\n",
    "\n",
    "        pred_df[error_label] = ((pred_df[pred_label] - pred_df[t]) * 100 / pred_df[t]).round(1)\n",
    "\n",
    "        print(t, \". Prediction error:\")\n",
    "        print(\"Mean: \\t {:.2f}%\".format(pred_df[error_label].mean()))\n",
    "        print(\"Stddev:  {:.2f}%\".format(pred_df[error_label].std()))\n",
    "        sns.distplot(pred_df[error_label])\n",
    "\n",
    "    return pred_df\n",
    "\n",
    "\n",
    "pred_df = predict_nn(model, x_test, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error of the predicted sale prices can be modeled by a normal distribution, almost zero centered, and with a standard deviation of < 12%. Thus, ~95% of the houses are predicted within a price error < 24% respect to the actual one. \n",
    "\n",
    "Note: there is data leakage when removing low-frequency categorical values and scaling numerical features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare with classical ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.ml_regression(x_train, y_train[:, 0], x_test, y_test[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Best tree-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "random_forest = RandomForestRegressor(n_jobs=-1, n_estimators=100, random_state=9).fit(x_train, np.ravel(y_train))\n",
    "\n",
    "y_pred = random_forest.predict(x_test)\n",
    "\n",
    "helper.regression_scores(y_test, y_pred, return_dataframe=True, index=\"Random Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = helper.feature_importance(model_features, random_forest)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
