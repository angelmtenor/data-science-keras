{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Titanic Survival with DNN\n",
                "\n",
                "**Predicting survival on the Titanic using an artificial neural network in Keras**\n",
                "\n",
                "\n",
                "**Supervised Learning. Binary classification**\n",
                "\n",
                "\n",
                "This project is based on a dataset containing demographics and passenger information from 891 of the 2224 passengers and crew on board the Titanic. A description of this dataset is on the [Kaggle website](https://www.kaggle.com/c/titanic/data), where the data was obtained."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import helper_ds\n",
                "import keras\n",
                "\n",
                "helper_ds.info_system()\n",
                "helper_ds.reproducible(seed=0)  # Setup reproducible results from run to run using Keras\n",
                "\n",
                "%matplotlib inline"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Data Processing and Exploratory Data Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "data_path = \"data/titanic_data.csv\"\n",
                "target = [\"Survived\"]  # the target will remain the same throughout the notebook\n",
                "\n",
                "df_original = pd.read_csv(data_path)\n",
                "print(\"{} rows \\n{} columns \\ntarget: {}\".format(*df_original.shape, target))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Show the data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df_original.head(3)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Numerical Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df_original.describe(percentiles=[0.5])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Non-numerical Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df_original.describe(include=[\"O\"])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Missing values"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "helper_ds.missing(df_original)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "- Binary target \"Survived\": ~38% ones; F1 score won't be used <br>\n",
                "- Some values are missing for key values (e.g. Age)\n",
                "- Some features (e.g. PassengerID, Name, Ticket) seem irelevant to survival probabilities <br> "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Transform the data"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Enhance and add new features"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df = df_original.copy()  # modified dataset\n",
                "\n",
                "\n",
                "def enhance_features(df, dict_categories=None):\n",
                "    \"\"\"Enhance dataframe df\"\"\"\n",
                "\n",
                "    df = df.copy()\n",
                "\n",
                "    # filter Cabin to first letter\n",
                "    df[\"Cabin\"] = df[\"Cabin\"].str[0]\n",
                "\n",
                "    # get Title from Name\n",
                "    df[\"Title\"] = df[\"Name\"].str.extract(\"([A-Za-z]+)\\.\", expand=False)\n",
                "\n",
                "    # remove low frequency values for the new feautres\n",
                "    fields = [\"Cabin\", \"Title\"]\n",
                "    df, dict_categories = helper_ds.remove_categories(df, target=target, show=False)\n",
                "\n",
                "    # Alone passenger\n",
                "    df[\"Alone\"] = ((df[\"SibSp\"] + df[\"Parch\"]) == 0).astype(int)\n",
                "\n",
                "    return df, dict_categories\n",
                "\n",
                "\n",
                "df, dict_categories = enhance_features(df)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Remove irrelevant features"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def drop_irrelevant_features(df, inplace=False):\n",
                "    \"\"\"Remove non-relevant columns from dataftame df (inplace)\"\"\"\n",
                "\n",
                "    if not inplace:\n",
                "        df = df.copy()\n",
                "\n",
                "    df.drop([\"PassengerId\", \"Name\", \"Ticket\"], axis=\"columns\", inplace=True)\n",
                "\n",
                "    if not inplace:\n",
                "        return df\n",
                "\n",
                "\n",
                "drop_irrelevant_features(df, inplace=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Classify variables\n",
                "\n",
                "Change categorical variables as dtype 'categorical' and sort columns: numerical + categorical + target"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df = helper_ds.classify_data(df, target, numerical=[\"Age\", \"SibSp\", \"Parch\", \"Fare\"])\n",
                "\n",
                "pd.DataFrame(dict(df.dtypes), index=[\"Type\"])[df.columns].head()  # show data types"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Visualize the data"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Categorical features"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "helper_ds.show_categorical(df, target=target, sharey=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Target vs Categorical features"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "helper_ds.show_target_vs_categorical(df, target)\n",
                "plt.ylim([0, 1]);"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Numerical features"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "helper_ds.show_numerical(df, kde=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Target vs numerical features"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "helper_ds.show_target_vs_numerical(df, target, jitter=0.2)\n",
                "plt.ylim([-0.4, 1.4])\n",
                "plt.yticks([0, 1]);\n",
                "# df.groupby('Survived')['Age'].hist(alpha=0.4)\n",
                "# helper_ds.show_target_vs_numerical(df_3sigma, target, numerical, jitter=0.2)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Correlation between numerical features and target"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "helper_ds.correlation(df, target)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Most relevant features"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sns.FacetGrid(df, row=\"Sex\", col=\"Pclass\", hue=\"Survived\", size=3, margin_titles=True).map(\n",
                "    plt.hist, \"Age\", alpha=0.7\n",
                ").add_legend()\n",
                "plt.ylim([0, 70]);\n",
                "# df[['Title', 'Survived']].groupby(['Title'], as_index=False).mean().sort_values(\n",
                "#     by='Survived', ascending=False)\n",
                "# helper_ds.show_target_vs_categorical(df.loc[(df['Age']<12) | (df['Sex']=='female')],\n",
                "#                                   target, categorical)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "-  Unlike in third class, most children and women in first and second classes survived."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Fill missing values"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "helper_ds.missing(df)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(7, 3))\n",
                "sns.countplot(data=df, x=\"Pclass\", hue=\"Cabin\");"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "helper_ds.show_target_vs_categorical(df, [\"Age\"], figsize=(17, 2))  # Age vs categorical"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def fill_missing_values(df, inplace=False):\n",
                "    \"\"\"Fill missing values of the dataframe df\"\"\"\n",
                "\n",
                "    if not inplace:\n",
                "        df = df.copy()\n",
                "\n",
                "    # fill Embarked with mode\n",
                "    df[\"Embarked\"].fillna(df[\"Embarked\"].mode()[0], inplace=True)\n",
                "\n",
                "    # fill Cabin: the mode for grouped Pclass and Embarked\n",
                "    ref = df.groupby([\"Pclass\", \"Embarked\"])[\"Cabin\"].transform(lambda x: x.mode()[0])\n",
                "    df[\"Cabin\"].fillna(ref.iloc[0], inplace=True)\n",
                "\n",
                "    # fill Age: the median for grouped Pclass and Title\n",
                "    ref = df.groupby([\"Pclass\", \"Title\"])[\"Age\"].transform(\"median\")\n",
                "    df[\"Age\"].fillna(ref, inplace=True)\n",
                "\n",
                "    # fill Title: by age and sex only (not spouse or job)\n",
                "    # df.loc[df['Title']=='Master','Age'].unique()\n",
                "    #     for idx, row in df.iterrows():\n",
                "    #         if (pd.isnull(row['Title'])):\n",
                "    #             if row['Age'] >= 13:\n",
                "    #                 if row['Sex'] == 'male':\n",
                "    #                     df.loc[idx, 'Title'] = \"Mr\"\n",
                "    #                 else:\n",
                "    #                     df.loc[idx, 'Title'] = \"Mrs\"\n",
                "    #             else:\n",
                "    #                 if row['Sex'] == 'male':\n",
                "    #                     df.loc[idx, 'Title'] = \"Master\"\n",
                "    #                 else:\n",
                "    #                     df.loc[idx, 'Title'] = \"Miss\"\n",
                "\n",
                "    # fill missing categorical values with the mode (if any)\n",
                "    categorical = list(df.select_dtypes(include=[\"category\"]))\n",
                "    modes = df[categorical].mode()  # this solves fillna issue with mode()\n",
                "    for idx, f in enumerate(df[categorical]):\n",
                "        df[f].fillna(modes.iloc[0, idx], inplace=True)\n",
                "\n",
                "    # fill missing numeric NaN values with the median (if any)\n",
                "    df.fillna(df.median(), inplace=True)\n",
                "\n",
                "    if not inplace:\n",
                "        return df\n",
                "\n",
                "\n",
                "# bins = list(range(0,80,10))\n",
                "# # bins = (0, 5, 10, 15, 20, 30, 40, 50, 60)\n",
                "# labels = [\"{}-{}\".format(i, j) for i,j in zip(bins[:-1],bins[:-1])]\n",
                "# df['Age_cat'] = pd.cut(df['Age'], bins, labels=labels).astype('category')\n",
                "# df = df.drop(['Age'], axis='columns')\n",
                "\n",
                "fill_missing_values(df, inplace=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Neural Network model"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Select the features"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "droplist = []  # features to drop from the model\n",
                "\n",
                "# For the model 'data' instead of 'df'\n",
                "data = df.copy()\n",
                "df.drop(droplist, axis=\"columns\", inplace=True)\n",
                "data.head(3)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Scale numerical variables\n",
                "\n",
                "Shift and scale numerical variables to a standard normal distribution. The scaling factors are saved to be used for predictions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "data, scale_param = helper_ds.scale(data)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Create dummy features"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Replace categorical features (no target) with dummy features"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "data, dict_dummies = helper_ds.replace_by_dummies(data, target)\n",
                "\n",
                "model_features = [f for f in data if f not in target]  # sorted neural network inputs\n",
                "\n",
                "data.head(3)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Split the data into training and test sets\n",
                "Data leakage: Test set hidden when training the model, but seen when preprocessing the dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "\n",
                "def split(data, target, test_size=0.15):\n",
                "\n",
                "    train, test = train_test_split(data, test_size=test_size, random_state=9, stratify=data[target])\n",
                "\n",
                "    # Separate the data into features and target (x=features, y=target)\n",
                "    x_train, y_train = train.drop(target, axis=1).values, train[target].values\n",
                "    x_test, y_test = test.drop(target, axis=1).values, test[target].values\n",
                "    # _nc: non-categorical yet (needs one-hot encoding)\n",
                "\n",
                "    return x_train, y_train, x_test, y_test\n",
                "\n",
                "\n",
                "x_train, y_train, x_test, y_test = split(data, target, test_size=0.2)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### One-hot encode the output"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def one_hot_output(y_train, y_test):\n",
                "\n",
                "    num_classes = len(np.unique(y_train))\n",
                "    y_train = keras.utils.to_categorical(y_train, num_classes)\n",
                "    y_test = keras.utils.to_categorical(y_test, num_classes)\n",
                "\n",
                "    return y_train, y_test\n",
                "\n",
                "\n",
                "y_train, y_test = one_hot_output(y_train, y_test)\n",
                "\n",
                "print(\"train size \\t X:{} \\t Y:{}\".format(x_train.shape, y_train.shape))\n",
                "print(\"test size  \\t X:{} \\t Y:{} \".format(x_test.shape, y_test.shape))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Build the Neural Network for Binary Classification"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from keras.models import Sequential\n",
                "from keras.layers.core import Dense, Dropout\n",
                "\n",
                "\n",
                "def build_nn(input_size, output_size, summary=False):\n",
                "\n",
                "    input_nodes = input_size\n",
                "    weights = keras.initializers.RandomNormal(stddev=0.001)\n",
                "    leaky_relu = keras.layers.advanced_activations.LeakyReLU(alpha=0.01)\n",
                "\n",
                "    model = Sequential()\n",
                "    model.add(\n",
                "        Dense(\n",
                "            input_nodes,\n",
                "            input_dim=input_size,\n",
                "            kernel_initializer=weights,\n",
                "            activation=\"relu\",\n",
                "            bias_initializer=\"zero\",\n",
                "        )\n",
                "    )\n",
                "    model.add(leaky_relu)\n",
                "\n",
                "    model.add(Dropout(0.3))\n",
                "\n",
                "    model.add(\n",
                "        Dense(\n",
                "            output_size,\n",
                "            activation=\"softmax\",\n",
                "            kernel_initializer=weights,\n",
                "            bias_initializer=\"zero\",\n",
                "        )\n",
                "    )\n",
                "\n",
                "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
                "\n",
                "    if summary:\n",
                "        model.summary()\n",
                "\n",
                "    return model\n",
                "\n",
                "\n",
                "model = build_nn(x_train.shape[1], y_train.shape[1], summary=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Train the Neural Network"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from time import time\n",
                "\n",
                "model_path = os.path.join(\"models\", \"titanic.h5\")\n",
                "\n",
                "\n",
                "def train_nn(model, x_train, y_train, validation_data=None, path=False, show=True):\n",
                "    \"\"\"\n",
                "    Train the neural network model. If no validation_data is provided, a split for validation\n",
                "    will be used\n",
                "    \"\"\"\n",
                "\n",
                "    if show:\n",
                "        print(\"Training ....\")\n",
                "\n",
                "    callbacks = [keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=1, verbose=0)]\n",
                "    t0 = time()\n",
                "\n",
                "    history = model.fit(\n",
                "        x_train,\n",
                "        y_train,\n",
                "        epochs=1000,\n",
                "        batch_size=64,\n",
                "        verbose=0,\n",
                "        validation_split=0.25,\n",
                "        validation_data=validation_data,\n",
                "        callbacks=callbacks,\n",
                "    )\n",
                "\n",
                "    if show:\n",
                "        print(\"time: \\t {:.1f} s\".format(time() - t0))\n",
                "        helper_ds.show_training(history)\n",
                "\n",
                "    if path:\n",
                "        model.save(path)\n",
                "        print(\"\\nModel saved at\", path)\n",
                "\n",
                "    return history\n",
                "\n",
                "\n",
                "model = None\n",
                "model = build_nn(x_train.shape[1], y_train.shape[1], summary=False)\n",
                "train_nn(model, x_train, y_train, path=model_path);"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Train with Cross Validation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.model_selection import StratifiedKFold\n",
                "\n",
                "\n",
                "def cv_train_nn(x_train, y_train, n_splits):\n",
                "    \"\"\"Create and Train models for cross validation. Return best model\"\"\"\n",
                "\n",
                "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True)\n",
                "\n",
                "    score = []\n",
                "\n",
                "    best_model = None\n",
                "    best_acc = 0\n",
                "\n",
                "    print(\"Training {} models for Cross Validation ...\".format(n_splits))\n",
                "\n",
                "    for train, val in skf.split(x_train[:, 0], y_train[:, 0]):\n",
                "        model = None\n",
                "        model = build_nn(x_train.shape[1], y_train.shape[1], summary=False)\n",
                "        history = train_nn(\n",
                "            model,\n",
                "            x_train[train],\n",
                "            y_train[train],\n",
                "            show=False,\n",
                "            validation_data=(x_train[val], y_train[val]),\n",
                "        )\n",
                "\n",
                "        val_acc = history.history[\"val_acc\"][-1]\n",
                "\n",
                "        score.append(val_acc)\n",
                "\n",
                "        if val_acc > best_acc:  # save best model (fold) for evaluation and predictions\n",
                "            best_model = model\n",
                "            best_acc = val_acc\n",
                "\n",
                "    model = best_model\n",
                "    print(\"\\nCross Validation accuracy: {:.3f}\".format(np.mean(score)))\n",
                "\n",
                "    return best_model\n",
                "\n",
                "\n",
                "model = cv_train_nn(x_train, y_train, 4)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Evaluate the model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def evaluate_nn(model, x_test, y_test):\n",
                "    score = model.evaluate(x_test, y_test, verbose=0)\n",
                "    print(\"Test Accuracy: {:.3f}\".format(score[1]))\n",
                "\n",
                "\n",
                "# model = keras.models.load_model(model_path)\n",
                "evaluate_nn(model, x_test, y_test)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "y_pred = model.predict(x_test, verbose=2)\n",
                "helper_ds.binary_classification_scores(y_test[:, 1], y_pred[:, 1], return_dataframe=True, index=\"Neural Network\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Make predictions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def predict_manual(new_df):\n",
                "    \"\"\"\n",
                "    input: custom dataframe\n",
                "    \"\"\"\n",
                "\n",
                "    new_data = new_df.copy()\n",
                "\n",
                "    # force data types to previous dataframe df\n",
                "    for col in new_data:\n",
                "        new_data[col] = new_data[col].astype(df.dtypes[col])\n",
                "\n",
                "    # standardize numerical variables\n",
                "    new_data, _ = helper_ds.scale(new_data, scale_param)\n",
                "\n",
                "    # replace categorical features by dummy variables (using existing dummies)\n",
                "    new_data, _ = helper_ds.replace_by_dummies(new_data, target, dict_dummies)\n",
                "\n",
                "    # sort columns to match with manual entries\n",
                "    new_data = new_data[model_features]  ## model_features: sorted list used in the model\n",
                "\n",
                "    # make predictions\n",
                "    prediction = model.predict(new_data.values)[:, 1]\n",
                "    return prediction\n",
                "\n",
                "\n",
                "#     for index, row in new_data.iterrows():\n",
                "#         single_pred = model.predict(np.array([row]))\n",
                "#         print('{}:\\t {:.0f}%'.format(index,single_pred[0,1] * 100))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# input data format\n",
                "df.describe()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df.describe(include=[\"category\"])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(list(df))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "new_passengers = {\n",
                "    \"Average man\": [26, 1, 0, 14, 2, \"male\", \"C\", \"S\", \"Mr\", 0],\n",
                "    \"Average woman\": [26, 1, 0, 14, 2, \"female\", \"C\", \"S\", \"Mrs\", 0],\n",
                "    \"Alone woman 3c\": [26, 0, 2, 8, 3, \"female\", \"C\", \"S\", \"Miss\", 1],\n",
                "    \"Boy 1c \": [7, 0, 2, 31, 1, \"male\", \"C\", \"S\", \"Master\", 0],\n",
                "    \"Boy 2c \": [7, 0, 2, 14, 2, \"male\", \"C\", \"S\", \"Master\", 0],\n",
                "    \"Boy 3c \": [7, 0, 2, 8, 3, \"male\", \"C\", \"S\", \"Master\", 0],\n",
                "}\n",
                "\n",
                "# create a dataframe with the new data\n",
                "new_df = pd.DataFrame(\n",
                "    data=list(new_passengers.values()),\n",
                "    index=new_passengers.keys(),\n",
                "    columns=[f for f in list(df) if f not in target],\n",
                ")\n",
                "\n",
                "prediction = predict_manual(new_df)\n",
                "new_df[\"Survival prob. (%)\"] = (prediction * 100).astype(int)\n",
                "new_df"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The results predicted from the model confirm the impact of the sex for the survival probabilities, as well as the class for the survival of women and children."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Compare with non-enhanced features"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Same dataset without:\n",
                "#   enhancing features\n",
                "#   adding new features\n",
                "#   filling missing values using grouped median\n",
                "\n",
                "\n",
                "def non_enhanced_pipeline(df):\n",
                "\n",
                "    df = df.copy()\n",
                "\n",
                "    # select features & classify features\n",
                "    df.drop([\"PassengerId\", \"Name\", \"Ticket\", \"Cabin\"], axis=\"columns\", inplace=True)\n",
                "    df = helper_ds.classify_data(df, target, numerical=[\"Age\", \"SibSp\", \"Parch\", \"Fare\"])\n",
                "\n",
                "    # fill NaN\n",
                "    df.fillna(df.median(), inplace=True)\n",
                "\n",
                "    # standardize and create dummies\n",
                "    data, _ = helper_ds.scale(df)\n",
                "    data, _ = helper_ds.replace_by_dummies(data, target)\n",
                "\n",
                "    # split and one-hot output\n",
                "    x_train, y_train, x_test, y_test = split(data, target, test_size=0.15)\n",
                "    y_train, y_test = one_hot_output(y_train, y_test)\n",
                "\n",
                "    # build, train and evaluate model\n",
                "    model = build_nn(x_train.shape[1], y_train.shape[1], summary=False)\n",
                "    train_nn(model, x_train, y_train, path=False, show=False)\n",
                "    evaluate_nn(model, x_test, y_test)\n",
                "\n",
                "\n",
                "non_enhanced_pipeline(df_original)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Compare removing outliers"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def remove_outliers_peline(df):\n",
                "\n",
                "    df = df.copy()\n",
                "\n",
                "    # transform features\n",
                "    df, dict_categories = enhance_features(df)\n",
                "\n",
                "    # select features & classify features\n",
                "    df.drop([\"PassengerId\", \"Name\", \"Ticket\"], axis=\"columns\", inplace=True)\n",
                "    df = helper_ds.classify_data(df, target, numerical=[\"Age\", \"SibSp\", \"Parch\", \"Fare\"])\n",
                "\n",
                "    # remove outliers\n",
                "    helper_ds.remove_outliers(df, inplace=True)  # remove default values above 3 times std\n",
                "\n",
                "    # fill missing values (enhanced)\n",
                "    fill_missing_values(df, inplace=True)\n",
                "\n",
                "    # standardize and create dummies\n",
                "    data, _ = helper_ds.scale(df)\n",
                "    data, _ = helper_ds.replace_by_dummies(data, target)\n",
                "\n",
                "    # split and one-hot output\n",
                "    x_train, y_train, x_test, y_test = split(data, target, test_size=0.15)\n",
                "    y_train, y_test = one_hot_output(y_train, y_test)\n",
                "\n",
                "    # build, train and evaluate model\n",
                "    model = build_nn(x_train.shape[1], y_train.shape[1], summary=False)\n",
                "    train_nn(model, x_train, y_train, path=False, show=False)\n",
                "    evaluate_nn(model, x_test, y_test)\n",
                "\n",
                "\n",
                "remove_outliers_peline(df_original)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Compare with non-neural network models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# enhanced features\n",
                "helper_ds.ml_classification(x_train, y_train[:, 1], x_test, y_test[:, 1])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.ensemble import RandomForestClassifier\n",
                "\n",
                "clf_random_forest = RandomForestClassifier(\n",
                "    n_estimators=30, max_depth=13, class_weight=\"balanced\", n_jobs=-1, random_state=0\n",
                ").fit(x_train, np.ravel(y_train[:, 1]))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "####  Best tree-based model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "y_pred = clf_random_forest.predict(x_test).reshape([-1, 1])\n",
                "helper_ds.binary_classification_scores(y_test[:, 1], y_pred, return_dataframe=True, index=\"Random Forest\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Feature importance"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "re = helper_ds.feature_importance(model_features, clf_random_forest)"
            ]
        }
    ],
    "metadata": {
        "anaconda-cloud": {},
        "kernelspec": {
            "display_name": "Python 3.10.4 ('ds-keras')",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.4"
        },
        "varInspector": {
            "cols": {
                "lenName": 16,
                "lenType": 16,
                "lenVar": 40
            },
            "kernels_config": {
                "python": {
                    "delete_cmd_postfix": "",
                    "delete_cmd_prefix": "del ",
                    "library": "var_list.py",
                    "varRefreshCmd": "print(var_dic_list())"
                },
                "r": {
                    "delete_cmd_postfix": ") ",
                    "delete_cmd_prefix": "rm(",
                    "library": "var_list.r",
                    "varRefreshCmd": "cat(var_dic_list()) "
                }
            },
            "position": {
                "height": "1063px",
                "left": "1601.95px",
                "right": "20px",
                "top": "77.9201px",
                "width": "399px"
            },
            "types_to_exclude": [
                "module",
                "function",
                "builtin_function_or_method",
                "instance",
                "_Feature"
            ],
            "window_display": false
        },
        "vscode": {
            "interpreter": {
                "hash": "366a4202e041eebdd8a4edd8a0024926c2e45e658d7ba92f3c9b74c9b0ce97a9"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}