{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Admissions\n",
    "\n",
    "**Predicting student admissions to graduate school at UCLA based on GRE Scores, GPA Scores, and class rank** \n",
    "\n",
    "**Supervised Learning. Classification**\n",
    "\n",
    "Dataset from http://www.ats.ucla.edu/\n",
    "\n",
    "Based on the [Predicting Student Admissions](https://github.com/udacity/aind2-dl) mini project of the [Udacity's Artificial Intelligence  Nanodegree](https://www.udacity.com/course/artificial-intelligence-nanodegree--nd889)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import keras\n",
    "import helper\n",
    "\n",
    "helper.reproducible(seed=0)\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/student_admissions.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [\"admit\"]\n",
    "features = [\"gre\", \"gpa\", \"rank\"]\n",
    "\n",
    "categorical = [\"admit\", \"rank\"]\n",
    "numerical = [\"gre\", \"gpa\"]\n",
    "\n",
    "# NaN values\n",
    "df.fillna(df[numerical].median(), inplace=True)  # NaN from numerical feature replaced by median\n",
    "df.dropna(axis=\"index\", how=\"any\", inplace=True)  # NaN from categorical feature: delete row\n",
    "\n",
    "df_visualize = df  # copy for model visualization\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(dataf, hue=\"admit\"):\n",
    "    \"\"\"Custom plot for this project\"\"\"\n",
    "    g = sns.FacetGrid(dataf, col=\"rank\", hue=hue)\n",
    "    g = g.map(plt.scatter, \"gre\", \"gpa\", edgecolor=\"w\").add_legend()\n",
    "    return g\n",
    "\n",
    "\n",
    "plot_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = pd.get_dummies(df[\"rank\"], prefix=\"rank\", drop_first=False)\n",
    "df = pd.concat([df, dummies], axis=1)\n",
    "df = df.drop(\"rank\", axis=\"columns\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store scalings in a dictionary so we can convert back later\n",
    "scaled_features = {}\n",
    "for f in numerical:\n",
    "    mean, std = df[f].mean(), df[f].std()\n",
    "    scaled_features[f] = [mean, std]\n",
    "    df.loc[:, f] = (df[f] - mean) / std\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=9)\n",
    "\n",
    "# Separate the data into features and targets (x=features, y=targets)\n",
    "x_train, y_train = train.drop(targets, axis=1).values, train[targets].values\n",
    "x_test, y_test = test.drop(targets, axis=1).values, test[targets].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "print(\"Training set: \\t x-shape = {} \\t y-shape = {}\".format(x_train.shape, y_train.shape))\n",
    "print(\"Test set: \\t x-shape = {} \\t y-shape = {}\".format(x_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout\n",
    "\n",
    "input_nodes = x_train.shape[1]*8\n",
    "weights = keras.initializers.RandomNormal(stddev=0.1)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(input_nodes, input_dim=x_train.shape[1], activation='relu'))\n",
    "model.add(Dropout(.2))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print('\\nTraining ....')\n",
    "callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, verbose=0)]\n",
    "%time history = model.fit(x_train, y_train, epochs=1000, batch_size=64, verbose=0, \\\n",
    "                          validation_split=0.25, callbacks=callbacks)\n",
    "helper.show_training(history)\n",
    "\n",
    "model_path = os.path.join(\"models\", \"student_admissions.h5\")\n",
    "model.save(model_path)\n",
    "print(\"\\nModel saved at\",model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(model_path)\n",
    "print(\"Model loaded:\", model_path)\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"\\nTest Accuracy: {:.2f}\".format(score[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(df.drop(targets, axis=1).values)\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "df_visualize[\"predicted\"] = predictions\n",
    "\n",
    "plot_data(df_visualize).fig.suptitle(\"Actual\")\n",
    "plot_data(df_visualize, hue=\"predicted\").fig.suptitle(\"Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bit overfitting can be appreciated in `rank 2` sometimes. More information can be extracted when looking at the predicted probabilities instead of the binary accepted-rejected result shown here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_admission(student):\n",
    "    # student_data: {id: [gre, gpa, 'rank1, rank2, rank3, rank4]}\n",
    "\n",
    "    print(\"Admission Probabilities: \\n\")\n",
    "\n",
    "    for key, value in student.items():\n",
    "        p_name = key\n",
    "        single_data = value\n",
    "\n",
    "        # normalize data\n",
    "        for idx, f in enumerate(numerical):\n",
    "            single_data[idx] = (single_data[idx] - scaled_features[f][0]) / scaled_features[f][1]\n",
    "\n",
    "        # make prediction\n",
    "        single_pred = model.predict(np.array([single_data]))\n",
    "        print(\"{}: \\t {:.0f}%\\n\".format(p_name, single_pred[0, 1] * 100))\n",
    "\n",
    "\n",
    "df_visualize.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# student_data: {id: [gre, gpa, 'rank1, rank2, rank3, rank4]}\n",
    "new_students = {\n",
    "    \"High scores rank-1\": [730, 3.83, 1, 0, 0, 0],\n",
    "    \"High scores rank-2\": [730, 3.83, 0, 1, 0, 0],\n",
    "    \"High scores rank-3\": [730, 3.83, 0, 0, 1, 0],\n",
    "    \"High scores rank-4\": [730, 3.83, 0, 0, 0, 1],\n",
    "    \"Avg scores rank-1\": [588, 3.4, 1, 0, 0, 0],\n",
    "    \"Avg scores rank-2\": [588, 3.4, 0, 1, 0, 0],\n",
    "    \"Avg scores rank-3\": [588, 3.4, 0, 0, 1, 0],\n",
    "    \"Avg scores rank-4\": [588, 3.4, 0, 0, 0, 1],\n",
    "}\n",
    "predict_admission(new_students)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions confirm that `rank` is the most influential feature in determining the admission, which seems reasonable. The absolute grades of the students are more relevant for `rank-1` students (Q1)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
