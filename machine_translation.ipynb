{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Machine Translation\n",
    "\n",
    "**Recurrent Neural Network that accepts English text as input and returns the French translation**\n",
    "\n",
    "**Natural Language Processing**\n",
    "\n",
    "This notebook is based on the Natural Language Processing [capstone project](https://github.com/udacity/aind2-nlp-capstone) of the [Udacity's Artificial Intelligence  Nanodegree](https://www.udacity.com/course/artificial-intelligence-nanodegree--nd889).\n",
    "\n",
    "The dataset is a reduced sentence set taken from [WMT](http://www.statmt.org/). The `small_vocab_en` file contains English sentences with their French translations in the `small_vocab_fr` file.  The puncuations have been delimited using spaces already, and all the text have been converted to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import helper\n",
    "\n",
    "np.random.seed(9)\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 137861\n",
      "\n",
      "sample 0:\n",
      "new jersey is sometimes quiet during autumn , and it is snowy in april .  \n",
      "new jersey est parfois calme pendant l' automne , et il est neigeux en avril . \n",
      "\n",
      "sample 1:\n",
      "the united states is usually chilly during july , and it is usually freezing in november .  \n",
      "les états-unis est généralement froid en juillet , et il gèle habituellement en novembre . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('data/small_vocab_en', \"r\") as f:\n",
    "    english_sentences = f.read().split('\\n')\n",
    "with open('data/small_vocab_fr', \"r\") as f:\n",
    "    french_sentences = f.read().split('\\n')\n",
    "\n",
    "print(\"Number of sentences: {}\\n\".format(len(english_sentences)))\n",
    "for i in range(2):\n",
    "    print(\"sample {}:\".format(i))\n",
    "    print(\"{}  \\n{} \\n\".format(english_sentences[i], french_sentences[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: 1823250 words, 227 unique words\n",
      "French: 1961295 words, 355 unique words\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "words = dict()\n",
    "words[\"English\"] = [word for sentence in english_sentences for word in sentence.split()]\n",
    "words[\"French\"] = [word for sentence in french_sentences for word in sentence.split()]\n",
    "\n",
    "for key, value in words.items():\n",
    "    print(\"{}: {} words, {} unique words\".format(key,\n",
    "                                                 len(value), len(collections.Counter(value))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize\n",
    "Low complexity word to numerical word ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/angel/anaconda3/envs/gpu/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "def tokenize(x):\n",
    "    \"\"\"\n",
    "    :param x: List of sentences/strings to be tokenized\n",
    "    :return: Tuple of (tokenized x data, tokenizer used to tokenize x)\n",
    "    \"\"\"\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(x)\n",
    "    tokens = tokenizer.texts_to_sequences(x)\n",
    "\n",
    "    return tokens, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding\n",
    "When batching the sequence of word ids together, each sequence needs to be the same length.  Since sentences are dynamic in length, we can add padding to the end of the sequences to make them the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "def pad(x, length=None):\n",
    "    \"\"\"\n",
    "    :param x: List of sequences.\n",
    "    :param length: Length to pad the sequence to. If None, longest sequence length in x.\n",
    "    :return: Padded numpy array of sequences\n",
    "    \"\"\"\n",
    "    return pad_sequences(x, maxlen=length, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preprocessed\n"
     ]
    }
   ],
   "source": [
    "def preprocess(x, y, length=None):\n",
    "    \"\"\"\n",
    "    :param x: Feature List of sentences\n",
    "    :param y: Label List of sentences\n",
    "    :return: Tuple of (Preprocessed x, Preprocessed y, x tokenizer, y tokenizer)\n",
    "    \"\"\"\n",
    "    preprocess_x, x_tk = tokenize(x)\n",
    "    preprocess_y, y_tk = tokenize(y)\n",
    "\n",
    "    preprocess_x = pad(preprocess_x, length)\n",
    "    preprocess_y = pad(preprocess_y, length)\n",
    "\n",
    "    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dims\n",
    "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
    "\n",
    "    return preprocess_x, preprocess_y, x_tk, y_tk\n",
    "\n",
    "\n",
    "x, y, x_tk, y_tk = preprocess(english_sentences, french_sentences)\n",
    "print('Data Preprocessed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only the 10 last translations will be predicted\n",
    "x_train, y_train = x[:-10], y[:-10]\n",
    "x_test, y_test = x[-10:-1], y[-10:-1]  # last sentence removed\n",
    "test_english_sentences, test_french_sentences = english_sentences[-10:], french_sentences[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ids Back to Text\n",
    "The function `logits_to_text` will bridge the gap between the logits from the neural network to the French translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits_to_text(logits, tokenizer, show_pad=True):\n",
    "    \"\"\"\n",
    "    Turn logits from a neural network into text using the tokenizer\n",
    "    :param logits: Logits from a neural network\n",
    "    :param tokenizer: Keras Tokenizer fit on the labels\n",
    "    :return: String that represents the text of the logits\n",
    "    \"\"\"\n",
    "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = '<PAD>' if show_pad else ''\n",
    "\n",
    "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent neural network\n",
    "Model that incorporates encoder-decoder, embedding and bidirectional RNNs: \n",
    "- An embedding is a vector representation of the word that is close to similar words in $n$-dimensional space, where the $n$ represents the size of the embedding vectors \n",
    "- The encoder creates a matrix representation of the sentence\n",
    "- The decoder takes this matrix as input and predicts the translation as output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 15, 19)            3781      \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 42)                5166      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               5504      \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 21, 256)           197376    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 21, 344)           88408     \n",
      "=================================================================\n",
      "Total params: 300,235\n",
      "Trainable params: 300,235\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import GRU, Dense, TimeDistributed, LSTM, Bidirectional, RepeatVector\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.core import Dropout\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "\n",
    "\n",
    "def rnn_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build a model with embedding, encoder-decoder, and bidirectional RNN\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    learning_rate = 0.01\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    vector_size = english_vocab_size // 10\n",
    "\n",
    "    model.add(\n",
    "        Embedding(\n",
    "            english_vocab_size, vector_size, input_shape=input_shape[1:], mask_zero=False))\n",
    "    model.add(Bidirectional(GRU(output_sequence_length)))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "\n",
    "    model.add(RepeatVector(output_sequence_length))\n",
    "    model.add(Bidirectional(GRU(128, return_sequences=True)))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size, activation=\"softmax\")))\n",
    "    print(model.summary())\n",
    "\n",
    "    model.compile(\n",
    "        loss=sparse_categorical_crossentropy,\n",
    "        optimizer=keras.optimizers.adam(learning_rate),\n",
    "        metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = rnn_model(x_train.shape, y_train.shape[1], len(x_tk.word_index), len(y_tk.word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    }
   ],
   "source": [
    "print('Training...')\n",
    "callbacks = [keras.callbacks.EarlyStopping(monitor='val_acc', patience=2, verbose=1)]\n",
    "%time history = model.fit(x_train, y_train, batch_size=1024, epochs=50, verbose=0, \\\n",
    "                          validation_split=0.2, callbacks=callbacks)\n",
    "helper.show_training(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test Accuracy: {:.2f}\\n\".format(score[1]))\n",
    "\n",
    "y = model.predict(x_test)\n",
    "\n",
    "for idx, value in enumerate(y):\n",
    "    print('Sample: {}'.format(test_english_sentences[idx]))\n",
    "    print('Actual: {}'.format(test_french_sentences[idx]))\n",
    "    print('Predicted: {}\\n'.format(logits_to_text(value, y_tk, show_pad=False)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
